{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2ha9OWxf0jw"
   },
   "source": [
    "*Unit 2, Sprint 3, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hTictxWYih7"
   },
   "source": [
    "# Permutation & Boosting\n",
    "\n",
    "- Get **permutation importances** for model interpretation and feature selection\n",
    "- Use xgboost for **gradient boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMejJg0w8v76"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the code cell below. You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab.\n",
    "\n",
    "Libraries:\n",
    "\n",
    "- category_encoders\n",
    "- [**eli5**](https://eli5.readthedocs.io/en/latest/)\n",
    "- matplotlib\n",
    "- numpy\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- [**xgboost**](https://xgboost.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "::\n",
       "\n",
       "  %capture [--no-stderr] [--no-stdout] [--no-display] [output]\n",
       "\n",
       "run the cell, capturing stdout, stderr, and IPython's rich display() calls.\n",
       "\n",
       "positional arguments:\n",
       "  output        The name of the variable in which to store output. This is a\n",
       "                utils.io.CapturedIO object with stdout/err attributes for the\n",
       "                text of the captured output. CapturedOutput also has a show()\n",
       "                method for displaying the output, and __call__ as well, so you\n",
       "                can use that to quickly display the output. If unspecified,\n",
       "                captured output is discarded.\n",
       "\n",
       "optional arguments:\n",
       "  --no-stderr   Don't capture stderr.\n",
       "  --no-stdout   Don't capture stdout.\n",
       "  --no-display  Don't capture IPython's rich display.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/IPython/core/magics/execution.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?%%capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BFQMky3CYih-"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://github.com/skhabiri/PredictiveModeling-AppliedModeling-u2s3/tree/master/data/'\n",
    "    !pip install category_encoders==2.*\n",
    "    !pip install eli5\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go back to Tanzania Waterpumps for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-TExplb_Slf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Merge train_features.csv & train_labels.csv\n",
    "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n",
    "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
    "\n",
    "# Read test_features.csv & sample_submission.csv\n",
    "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
    "\n",
    "\n",
    "# Split train into train & val\n",
    "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
    "                              stratify=train['status_group'], random_state=42)\n",
    "\n",
    "\n",
    "def wrangle(X):\n",
    "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
    "    \n",
    "    # Prevent SettingWithCopyWarning\n",
    "    X = X.copy()\n",
    "    \n",
    "    # About 3% of the time, latitude has small values near zero,\n",
    "    # outside Tanzania, so we'll treat these values like zero.\n",
    "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
    "    \n",
    "    # When columns have zeros and shouldn't, they are like null values.\n",
    "    # So we will replace the zeros with nulls, and impute missing values later.\n",
    "    # Also create a \"missing indicator\" column, because the fact that\n",
    "    # values are missing may be a predictive signal.\n",
    "    cols_with_zeros = ['longitude', 'latitude', 'construction_year', \n",
    "                       'gps_height', 'population']\n",
    "    for col in cols_with_zeros:\n",
    "        X[col] = X[col].replace(0, np.nan)\n",
    "        # after imputing we will lose np.nan information\n",
    "        X[col+'_MISSING'] = X[col].isnull()\n",
    "            \n",
    "    # Drop duplicate columns\n",
    "    duplicates = ['quantity_group', 'payment_type']\n",
    "    X = X.drop(columns=duplicates)\n",
    "    \n",
    "    # Drop recorded_by (never varies) and id (always varies, random)\n",
    "    unusable_variance = ['recorded_by', 'id']\n",
    "    X = X.drop(columns=unusable_variance)\n",
    "    \n",
    "    # Convert date_recorded to datetime\n",
    "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
    "    \n",
    "    # Extract components from date_recorded, then drop the original column\n",
    "    X['year_recorded'] = X['date_recorded'].dt.year\n",
    "    X['month_recorded'] = X['date_recorded'].dt.month\n",
    "    X['day_recorded'] = X['date_recorded'].dt.day\n",
    "    X = X.drop(columns='date_recorded')\n",
    "    \n",
    "    # Engineer feature: how many years from construction_year to date_recorded\n",
    "    X['years'] = X['year_recorded'] - X['construction_year']\n",
    "    X['years_MISSING'] = X['years'].isnull()\n",
    "    \n",
    "    # return the wrangled dataframe\n",
    "    return X\n",
    "\n",
    "train = wrangle(train)\n",
    "val = wrangle(val)\n",
    "test = wrangle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhg8PQKt_jzP"
   },
   "outputs": [],
   "source": [
    "# Arrange data into X features matrix and y target vector\n",
    "target = 'status_group'\n",
    "X_train = train.drop(columns=target)\n",
    "y_train = train[target]\n",
    "X_val = val.drop(columns=target)\n",
    "y_val = val[target]\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m8lB4z5l_eml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.8135521885521886\n"
     ]
    }
   ],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median'), \n",
    "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# Fit on train, score on val\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get permutation importances for model interpretation and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Feature Importances are fast, but Permutation Importances may be more accurate.\n",
    "\n",
    "These links go deeper with explanations and examples:\n",
    "\n",
    "- Permutation Importances\n",
    "  - [Kaggle / Dan Becker: Machine Learning Explainability](https://www.kaggle.com/dansbecker/permutation-importance)\n",
    "  - [Christoph Molnar: Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
    "- (Default) Feature Importances\n",
    "  - [Ando Saabas: Selecting good features, Part 3, Random Forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
    "  - [Terence Parr, et al: Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HOayKBOYiit"
   },
   "source": [
    "There are three types of feature importances:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4bRhsxENYiiu"
   },
   "source": [
    "### 1. (Default) Feature Importances\n",
    "\n",
    "Fastest, good for first estimates, but be aware:\n",
    "\n",
    "\n",
    "\n",
    ">**When the dataset has two (or more) correlated features, then from the point of view of the model, any of these correlated features can be used as the predictor, with no concrete preference of one over the others.** But once one of them is used, the importance of others is significantly reduced since effectively the impurity they can remove is already removed by the first feature. As a consequence, they will have a lower reported importance. This is not an issue when we want to use feature selection to reduce overfitting, since it makes sense to remove features that are mostly duplicated by other features. But when interpreting the data, it can lead to the incorrect conclusion that one of the variables is a strong predictor while the others in the same group are unimportant, while actually they are very close in terms of their relationship with the response variable. — [Selecting good features – Part III: random forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/) \n",
    "\n",
    "\n",
    " \n",
    " > **The scikit-learn Random Forest feature importance ... tends to inflate the importance of continuous or high-cardinality categorical variables.** ... Breiman and Cutler, the inventors of Random Forests, indicate that this method of “adding up the gini decreases for each individual variable over all trees in the forest gives a **fast** variable importance that is often very consistent with the permutation importance measure.” —  [Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNVm6f7mYiiu"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get feature importances\n",
    "rf = pipeline.named_steps['randomforestclassifier']\n",
    "importances = pd.Series(rf.feature_importances_, X_train.columns)\n",
    "\n",
    "# Plot feature importances\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 20\n",
    "plt.figure(figsize=(10,n/2))\n",
    "plt.title(f'Top {n} features')\n",
    "importances.sort_values()[-n:].plot.barh(color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8HzLcCBYiiv"
   },
   "source": [
    "### 2. Drop-Column Importance\n",
    "\n",
    "The best in theory, but too slow in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQAOlERnYiiw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy without quantity: 0.7771043771043771\n",
      "Validation Accuracy with quantity: 0.8135521885521886\n",
      "Drop-Column Importance for quantity: 0.03644781144781151\n"
     ]
    }
   ],
   "source": [
    "column  = 'quantity'\n",
    "\n",
    "# Fit without column\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median'), \n",
    "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "pipeline.fit(X_train.drop(columns=column), y_train)\n",
    "score_without = pipeline.score(X_val.drop(columns=column), y_val)\n",
    "print(f'Validation Accuracy without {column}: {score_without}')\n",
    "\n",
    "# Fit with column\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median'), \n",
    "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "score_with = pipeline.score(X_val, y_val)\n",
    "print(f'Validation Accuracy with {column}: {score_with}')\n",
    "\n",
    "# Compare the error with & without column\n",
    "print(f'Drop-Column Importance for {column}: {score_with - score_without}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Vu39wGkYiix"
   },
   "source": [
    "### 3. Permutation Importance\n",
    "\n",
    "Permutation Importance is a good compromise between Feature Importance based on impurity reduction (which is the fastest) and Drop Column Importance (which is the \"best.\")\n",
    "\n",
    "[The ELI5 library documentation explains,](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)\n",
    "\n",
    "> Importance can be measured by looking at how much the score (accuracy, F1, R^2, etc. - any score we’re interested in) decreases when a feature is not available.\n",
    ">\n",
    "> To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. ...\n",
    ">\n",
    ">To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn’t work as-is, because estimators expect feature to be present. So instead of removing a feature we can replace it with random noise - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the same distribution as original feature values (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples’ feature values - this is how permutation importance is computed.\n",
    ">\n",
    ">The method is most suitable for computing feature importances when a number of columns (features) is not huge; it can be resource-intensive otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GYCiEx7zYiiy"
   },
   "source": [
    "### Do-It-Yourself way, for intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TksOf_n2Yiiy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3290     insufficient\n",
       "47666    insufficient\n",
       "2538           enough\n",
       "53117          enough\n",
       "51817          enough\n",
       "Name: quantity, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = 'quantity'\n",
    "X_val[feature].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enough          6619\n",
       "insufficient    2976\n",
       "dry             1325\n",
       "seasonal         806\n",
       "unknown          154\n",
       "Name: quantity, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[feature].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enough          0.557155\n",
       "insufficient    0.250505\n",
       "dry             0.111532\n",
       "seasonal        0.067845\n",
       "unknown         0.012963\n",
       "Name: quantity, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[feature].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3290              dry\n",
       "47666        seasonal\n",
       "2538     insufficient\n",
       "53117    insufficient\n",
       "51817    insufficient\n",
       "Name: quantity, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_permuted = X_val.copy()\n",
    "# If `x` is a multi-dimensional array, it is only shuffled along its first index.\n",
    "X_val_permuted[feature] = np.random.permutation(X_val[feature])\n",
    "X_val_permuted[feature].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enough          0.557155\n",
       "insufficient    0.250505\n",
       "dry             0.111532\n",
       "seasonal        0.067845\n",
       "unknown         0.012963\n",
       "Name: quantity, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_permuted[feature].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*np.random.permutation() only shuffles the value but the value_counts() remain the same*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with quantity: 0.8135521885521886\n",
      "Validation Accuracy with quantity permuted: 0.7133838383838383\n",
      "Permutation Importance: 0.10016835016835024\n"
     ]
    }
   ],
   "source": [
    "# Get the permutation importance\n",
    "# Notice that we don't need to refit here!\n",
    "score_permuted = pipeline.score(X_val_permuted, y_val)\n",
    "\n",
    "print(f'Validation Accuracy with {feature}: {score_with}')\n",
    "print(f'Validation Accuracy with {feature} permuted: {score_permuted}')\n",
    "print(f'Permutation Importance: {score_with - score_permuted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with wpt_name: 0.8135521885521886\n",
      "Validation Accuracy with wpt_name permuted: 0.8104377104377104\n",
      "Permutation Importance: 0.003114478114478181\n"
     ]
    }
   ],
   "source": [
    "# Rerun the permutation importance process, \n",
    "# but for a different feature\n",
    "feature = 'wpt_name'\n",
    "X_val_permuted = X_val.copy()\n",
    "X_val_permuted[feature] = np.random.permutation(X_val[feature])\n",
    "score_permuted = pipeline.score(X_val_permuted, y_val)\n",
    "\n",
    "print(f'Validation Accuracy with {feature}: {score_with}')\n",
    "print(f'Validation Accuracy with {feature} permuted: {score_permuted}')\n",
    "print(f'Permutation Importance: {score_with - score_permuted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LYk19SNYii7"
   },
   "source": [
    "### With eli5 library\n",
    "\n",
    "For more documentation on using this library, see:\n",
    "- [eli5.sklearn.PermutationImportance](https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#eli5.sklearn.permutation_importance.PermutationImportance)\n",
    "- [eli5.show_weights](https://eli5.readthedocs.io/en/latest/autodocs/eli5.html#eli5.show_weights)\n",
    "- [scikit-learn user guide, `scoring` parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)\n",
    "\n",
    "**eli5 doesn't work with pipelines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpSemTkFFP8i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47520, 45) (47520, 45)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "transformers = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    SimpleImputer(strategy='median')\n",
    ")\n",
    "\n",
    "# unlike .fit(), .fit_transform() returns the transformed input ndarray\n",
    "X_train_transformed = transformers.fit_transform(X_train)\n",
    "X_val_transformed = transformers.transform(X_val)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106143 0\n",
      "(47520, 45) (47520, 45)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.542898</td>\n",
       "      <td>-9.174777</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2049.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.665760</td>\n",
       "      <td>-9.308548</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>38.238568</td>\n",
       "      <td>-6.179919</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.716727</td>\n",
       "      <td>-1.289055</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35.389331</td>\n",
       "      <td>-6.399942</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1       2    3          4         5    6    7    8    9   ...   35  \\\n",
       "0    0.0  1.0  1167.0  1.0  33.542898 -9.174777  1.0  0.0  1.0  1.0  ...  0.0   \n",
       "1  500.0  2.0  2049.0  2.0  34.665760 -9.308548  2.0  0.0  2.0  2.0  ...  0.0   \n",
       "2   25.0  3.0   290.0  3.0  38.238568 -6.179919  3.0  0.0  3.0  3.0  ...  0.0   \n",
       "3    0.0  4.0  1167.0  4.0  30.716727 -1.289055  4.0  0.0  4.0  4.0  ...  0.0   \n",
       "4    0.0  5.0  1167.0  5.0  35.389331 -6.399942  5.0  0.0  5.0  5.0  ...  0.0   \n",
       "\n",
       "    36   37   38   39      40   41    42    43   44  \n",
       "0  0.0  1.0  1.0  1.0  2011.0  7.0  27.0  13.0  1.0  \n",
       "1  0.0  0.0  0.0  0.0  2011.0  3.0  23.0   3.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  2011.0  3.0   7.0   1.0  0.0  \n",
       "3  0.0  1.0  1.0  1.0  2011.0  7.0  31.0  13.0  1.0  \n",
       "4  0.0  1.0  1.0  1.0  2011.0  3.0  10.0  13.0  1.0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.isna().sum().sum(), pd.DataFrame(X_train_transformed).isna().sum().sum())\n",
    "print(X_train.shape, X_train_transformed.shape)\n",
    "pd.DataFrame(X_train_transformed).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahnam/.pyenv/versions/3.7.8/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/shahnam/.pyenv/versions/3.7.8/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PermutationImportance(estimator=RandomForestClassifier(n_jobs=-1,\n",
       "                                                       random_state=42),\n",
       "                      random_state=42, scoring='accuracy')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\"\"\"\n",
    "cv='prefit': After the fitting\n",
    "   ``feature_importances_`` attribute becomes available, but the estimator\n",
    "   itself is not fit again. \n",
    "n_iter : int, default 5\n",
    "    Number of random shuffle iterations. Decrease to improve speed,\n",
    "    increase to get more precise estimates.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "permuter = PermutationImportance(\n",
    "    model,\n",
    "    scoring='accuracy',\n",
    "    cv='prefit',\n",
    "    n_iter=5,     # Shuffles every features 5 times before averaging its permutation importance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fitting it with X_train_transformed could yield slightly different results.\n",
    "permuter.fit(X_val_transformed, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quantity                     0.101633\n",
       "amount_tsh                   0.010758\n",
       "waterpoint_type              0.010354\n",
       "extraction_type_class        0.010168\n",
       "longitude                    0.008838\n",
       "waterpoint_type_group        0.006751\n",
       "population                   0.006414\n",
       "latitude                     0.006246\n",
       "payment                      0.003064\n",
       "subvillage                   0.003030\n",
       "years                        0.002980\n",
       "public_meeting               0.002963\n",
       "construction_year            0.002609\n",
       "district_code                0.002239\n",
       "extraction_type_group        0.001785\n",
       "gps_height                   0.001768\n",
       "source                       0.001549\n",
       "day_recorded                 0.001498\n",
       "funder                       0.001128\n",
       "month_recorded               0.001128\n",
       "wpt_name                     0.000943\n",
       "region                       0.000926\n",
       "region_code                  0.000892\n",
       "longitude_MISSING            0.000875\n",
       "scheme_name                  0.000808\n",
       "permit                       0.000774\n",
       "lga                          0.000724\n",
       "scheme_management            0.000657\n",
       "ward                         0.000539\n",
       "water_quality                0.000539\n",
       "extraction_type              0.000539\n",
       "years_MISSING                0.000471\n",
       "management                   0.000455\n",
       "num_private                  0.000337\n",
       "source_class                 0.000337\n",
       "year_recorded                0.000269\n",
       "population_MISSING           0.000202\n",
       "latitude_MISSING             0.000135\n",
       "gps_height_MISSING           0.000051\n",
       "construction_year_MISSING    0.000051\n",
       "source_type                 -0.000017\n",
       "installer                   -0.000051\n",
       "management_group            -0.000488\n",
       "quality_group               -0.000572\n",
       "basin                       -0.001431\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = X_val.columns.tolist()\n",
    "pd.Series(data=permuter.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "# pd.Series(data=permuter.feature_importances_, index=feature_names).sort_values(ascending=False).plot.barh(color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To show an explanation of estimator parameters (weights) as an IPython.display.HTML object:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.1016\n",
       "                \n",
       "                    &plusmn; 0.0029\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                quantity\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.85%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0108\n",
       "                \n",
       "                    &plusmn; 0.0024\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                amount_tsh\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.96%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0104\n",
       "                \n",
       "                    &plusmn; 0.0018\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                waterpoint_type\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.01%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0102\n",
       "                \n",
       "                    &plusmn; 0.0015\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                extraction_type_class\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 96.38%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0088\n",
       "                \n",
       "                    &plusmn; 0.0015\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                longitude\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0068\n",
       "                \n",
       "                    &plusmn; 0.0018\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                waterpoint_type_group\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.11%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0064\n",
       "                \n",
       "                    &plusmn; 0.0008\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                population\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 97.16%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0062\n",
       "                \n",
       "                    &plusmn; 0.0026\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                latitude\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.28%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0031\n",
       "                \n",
       "                    &plusmn; 0.0010\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                payment\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.29%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0030\n",
       "                \n",
       "                    &plusmn; 0.0015\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                subvillage\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.31%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0030\n",
       "                \n",
       "                    &plusmn; 0.0026\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                years\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.32%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0030\n",
       "                \n",
       "                    &plusmn; 0.0011\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                public_meeting\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.46%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0026\n",
       "                \n",
       "                    &plusmn; 0.0029\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                construction_year\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.62%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0022\n",
       "                \n",
       "                    &plusmn; 0.0016\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                district_code\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.82%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0018\n",
       "                \n",
       "                    &plusmn; 0.0015\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                extraction_type_group\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.83%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0018\n",
       "                \n",
       "                    &plusmn; 0.0010\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                gps_height\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.93%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0015\n",
       "                \n",
       "                    &plusmn; 0.0011\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                source\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 98.96%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0015\n",
       "                \n",
       "                    &plusmn; 0.0017\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                day_recorded\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.14%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0011\n",
       "                \n",
       "                    &plusmn; 0.0012\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                funder\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.14%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0011\n",
       "                \n",
       "                    &plusmn; 0.0019\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                month_recorded\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.24%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0009\n",
       "                \n",
       "                    &plusmn; 0.0014\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                wpt_name\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.25%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0009\n",
       "                \n",
       "                    &plusmn; 0.0013\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                region\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.27%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0009\n",
       "                \n",
       "                    &plusmn; 0.0014\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                region_code\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.28%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0009\n",
       "                \n",
       "                    &plusmn; 0.0007\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                longitude_MISSING\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.32%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0008\n",
       "                \n",
       "                    &plusmn; 0.0020\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                scheme_name\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.34%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0008\n",
       "                \n",
       "                    &plusmn; 0.0006\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                permit\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.37%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0007\n",
       "                \n",
       "                    &plusmn; 0.0009\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                lga\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.41%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0007\n",
       "                \n",
       "                    &plusmn; 0.0020\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                scheme_management\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.49%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0017\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                ward\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.49%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0008\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                water_quality\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.49%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0014\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                extraction_type\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.53%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                years_MISSING\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.55%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0005\n",
       "                \n",
       "                    &plusmn; 0.0021\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                management\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.63%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                num_private\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.63%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0011\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                source_class\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.69%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0003\n",
       "                \n",
       "                    &plusmn; 0.0011\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                year_recorded\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.74%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0002\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                population_MISSING\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.81%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0001\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                latitude_MISSING\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.90%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0007\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                gps_height_MISSING\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.90%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0001\n",
       "                \n",
       "                    &plusmn; 0.0008\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                construction_year_MISSING\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 99.95%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                -0.0000\n",
       "                \n",
       "                    &plusmn; 0.0005\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                source_type\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 99.90%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                -0.0001\n",
       "                \n",
       "                    &plusmn; 0.0020\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                installer\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 99.52%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                -0.0005\n",
       "                \n",
       "                    &plusmn; 0.0006\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                management_group\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 99.47%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                -0.0006\n",
       "                \n",
       "                    &plusmn; 0.0012\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                quality_group\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, 98.99%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                -0.0014\n",
       "                \n",
       "                    &plusmn; 0.0009\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                basin\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_weights(\n",
    "    permuter,\n",
    "    top=None,\n",
    "    feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q07yW9k-Yii8"
   },
   "source": [
    "### We can use importances for feature selection\n",
    "\n",
    "For example, we can remove features with zero importance. The model trains faster and the score does not decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZrPFyEMYii9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before removing features: (47520, 45)\n",
      "Shape after removing features: (47520, 40)\n"
     ]
    }
   ],
   "source": [
    "print('Shape before removing features:', X_train.shape)\n",
    "\n",
    "minimum_importance = 0\n",
    "# mask is a ndarray with no column label but correct index order\n",
    "mask = permuter.feature_importances_ > minimum_importance\n",
    "features = X_train.columns[mask]\n",
    "X_train_trim = X_train[features]\n",
    "print('Shape after removing features:', X_train_trim.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fl67bCR7WY6j"
   },
   "source": [
    "# Use xgboost for gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Random Forest lesson, you learned this advice:\n",
    "\n",
    "#### Try Tree Ensembles when you do machine learning with labeled, tabular data\n",
    "- \"Tree Ensembles\" means Random Forest or **Gradient Boosting** models. \n",
    "- [Tree Ensembles often have the best predictive accuracy](https://arxiv.org/abs/1708.05070) with labeled, tabular data.\n",
    "- Why? Because trees can fit non-linear, non-[monotonic](https://en.wikipedia.org/wiki/Monotonic_function) relationships, and [interactions](https://christophm.github.io/interpretable-ml-book/interaction.html) between features.\n",
    "- A single decision tree, grown to unlimited depth, will [overfit](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/). We solve this problem by ensembling trees, with bagging (Random Forest) or **[boosting](https://www.youtube.com/watch?v=GM3CDQfQ4sw)** (Gradient Boosting).\n",
    "- Random Forest's advantage: may be less sensitive to hyperparameters. **Gradient Boosting's advantage:** may get better predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Random Forest, Gradient Boosting uses ensembles of trees. But the details of the ensembling technique are different:\n",
    "\n",
    "### Understand the difference between boosting & bagging\n",
    "\n",
    "Boosting (used by Gradient Boosting) is different than Bagging (used by Random Forests). \n",
    "\n",
    "Here's an excerpt from [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) Chapter 8.2.3, Boosting:\n",
    "\n",
    ">Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.\n",
    ">\n",
    ">**Boosting works in a similar way, except that the trees are grown _sequentially_: each tree is grown using information from previously grown trees.**\n",
    ">\n",
    ">Unlike fitting a single large decision tree to the data, which amounts to _fitting the data hard_ and potentially overfitting, the boosting approach instead _learns slowly._ Given the current model, we fit a decision tree to the residuals from the model.\n",
    ">\n",
    ">We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes. **By fitting small trees to the residuals, we slowly improve fˆ in areas where it does not perform well.**\n",
    ">\n",
    ">Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.\n",
    "\n",
    "This high-level overview is all you need to know for now. If you want to go deeper, we recommend you watch the StatQuest videos on gradient boosting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write some code. We have lots of options for which libraries to use:\n",
    "\n",
    "#### Python libraries for Gradient Boosting\n",
    "- [scikit-learn Gradient Tree Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) — slower than other libraries, but [the new version may be better](https://twitter.com/amuellerml/status/1129443826945396737)\n",
    "  - Anaconda: already installed\n",
    "  - Google Colab: already installed\n",
    "- [xgboost](https://xgboost.readthedocs.io/en/latest/) — can accept missing values and enforce [monotonic constraints](https://xiaoxiaowang87.github.io/monotonicity_constraint/)\n",
    "  - Anaconda, Mac/Linux: `conda install -c conda-forge xgboost`\n",
    "  - Windows: `conda install -c anaconda py-xgboost`\n",
    "  - Google Colab: already installed\n",
    "- [LightGBM](https://lightgbm.readthedocs.io/en/latest/) — can accept missing values and enforce [monotonic constraints](https://blog.datadive.net/monotonicity-constraints-in-machine-learning/)\n",
    "  - Anaconda: `conda install -c conda-forge lightgbm`\n",
    "  - Google Colab: already installed\n",
    "- [CatBoost](https://catboost.ai/) — can accept missing values and use [categorical features](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html) without preprocessing\n",
    "  - Anaconda: `conda install -c conda-forge catboost`\n",
    "  - Google Colab: `pip install catboost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, you'll use a new library, xgboost — But it has an API that's almost the same as scikit-learn, so it won't be a hard adjustment!\n",
    "\n",
    "#### [XGBoost Python API Reference: Scikit-Learn API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsnJRKjfWYph"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahnam/.pyenv/versions/3.7.8/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:07:53] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ordinalencoder',\n",
       "                 OrdinalEncoder(cols=['funder', 'installer', 'wpt_name',\n",
       "                                      'basin', 'subvillage', 'region', 'lga',\n",
       "                                      'ward', 'public_meeting',\n",
       "                                      'scheme_management', 'scheme_name',\n",
       "                                      'permit', 'extraction_type',\n",
       "                                      'extraction_type_group',\n",
       "                                      'extraction_type_class', 'management',\n",
       "                                      'management_group', 'payment',\n",
       "                                      'water_quality', 'quality_group',\n",
       "                                      'quantity', 'source',...\n",
       "                               importance_type='gain',\n",
       "                               interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_delta_step=0,\n",
       "                               max_depth=6, min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=-1, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', random_state=42,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "                               subsample=1, tree_method='exact',\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# xgboost does not need imputer\n",
    "pipeline = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.8026094276094277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = pipeline.predict(X_val)\n",
    "print('Validation Accuracy', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCjVSlD_XJr2"
   },
   "source": [
    "#### [Avoid Overfitting By Early Stopping With XGBoost In Python](https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/)\n",
    "\n",
    "Why is early stopping better than a For loop, or GridSearchCV, to optimize `n_estimators` in boosting ensembeled trees?\n",
    "\n",
    "With early stopping, if `n_iterations` is our number of iterations, then we fit `n_iterations` decision trees.\n",
    "\n",
    "With a for loop, or GridSearchCV, we'd fit `sum(range(1,n_rounds+1))` trees, as boosting is a series operation and requires everytime to start from one tree and goes up\n",
    "\n",
    "**But it doesn't work well with pipelines.** You may need to re-run multiple times with different values of other parameters such as `max_depth` and `learning_rate`.\n",
    "\n",
    "#### XGBoost parameters\n",
    "- [Notes on parameter tuning](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html)\n",
    "- [Parameters documentation](https://xgboost.readthedocs.io/en/latest/parameter.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Ensemble Boosting:\n",
    "RandomForest works based on bagging the input data by sampling the rows of input data randomly with/out replacement. Hence all the estimator trees can be trained at the same time. \n",
    "\n",
    "Gradient boost develops the trees sequentioally. It starts with the base model. Let's call the base_model_prediction (such as mean of all $y_{truth}$) as first estimate, $h_0$. Then create 1st Residual from that, by let's say $R_0=y_{true} - h_0$. Next, train the 1st tree for the residual label, $R_0$, as a target label. The predicted output of the first tree is an estimate of $R_0$, named $R0$. Adding predicted residual $R0$ to $h_0$ would come very close to the $y_{true}$. However, we don't want to take a big step, as it would introduce big variance despite offering small bias. Hence we multiply the predicted psudo residual $R0$ with learning rate $0<\\alpha<1$ to bring the the y prediction one step closer to the $y_{true}$. The new target prediction is $h_1 = h_0 + \\alpha \\times R0$. Now we can calculate the new psudo residual $R_1$ and then train a subsequent shallow tree for that, $R_1 = y_{true} - h_1$. Normally, $R_1$ is smaller residual than $R_0$ as we take small steps towards $y_{truth}$. This is because we added a portion of the predicted previouse residual to our previous total prediction and improved the new total prediction. Let's name the estimated $R_1$ as $R1$. Now the new improved target prediction is $h_2 = h_1 + \\alpha \\times R1 = h_0 + \\alpha \\times R0 + \\alpha \\times R1$. We continue this process till we statisfy loss function criteria.\n",
    "\n",
    "In a nutshell we start with a base model and derive the psudo residual. Then train a shallow tree to predict that residual and add a fraction of that tree's predicted residual label as its contribution to the base model to improve our y prediction. Next calculate the newly improved residual based on the $y_{true}$ and the previous total prediction. The second tree would try to predict the new residual and add a fractional contribution (learning rate) to the previousely overall prediction. By training sequential trees to predict every newly improved residual and improving the overal prediction by small steps controlled by learning rate we get closer to the $y_{true}$. Once again need to emphasize that every new residual is calculated based on $y_{true}$ - **overall** prediction calculated in its previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNX3IKftXBFS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-merror:0.24956\tvalidation_1-merror:0.25800\n",
      "[1]\tvalidation_0-merror:0.23944\tvalidation_1-merror:0.25050\n",
      "[2]\tvalidation_0-merror:0.23163\tvalidation_1-merror:0.24268\n",
      "[3]\tvalidation_0-merror:0.22525\tvalidation_1-merror:0.23763\n",
      "[4]\tvalidation_0-merror:0.21871\tvalidation_1-merror:0.23451\n",
      "[5]\tvalidation_0-merror:0.21212\tvalidation_1-merror:0.23014\n",
      "[6]\tvalidation_0-merror:0.20572\tvalidation_1-merror:0.22660\n",
      "[7]\tvalidation_0-merror:0.19844\tvalidation_1-merror:0.22298\n",
      "[8]\tvalidation_0-merror:0.19512\tvalidation_1-merror:0.22222\n",
      "[9]\tvalidation_0-merror:0.19226\tvalidation_1-merror:0.22087\n",
      "[10]\tvalidation_0-merror:0.18872\tvalidation_1-merror:0.21785\n",
      "[11]\tvalidation_0-merror:0.18468\tvalidation_1-merror:0.21406\n",
      "[12]\tvalidation_0-merror:0.18039\tvalidation_1-merror:0.21195\n",
      "[13]\tvalidation_0-merror:0.17837\tvalidation_1-merror:0.21212\n",
      "[14]\tvalidation_0-merror:0.17572\tvalidation_1-merror:0.21052\n",
      "[15]\tvalidation_0-merror:0.17323\tvalidation_1-merror:0.21254\n",
      "[16]\tvalidation_0-merror:0.16955\tvalidation_1-merror:0.21145\n",
      "[17]\tvalidation_0-merror:0.16608\tvalidation_1-merror:0.21103\n",
      "[18]\tvalidation_0-merror:0.16475\tvalidation_1-merror:0.21187\n",
      "[19]\tvalidation_0-merror:0.16206\tvalidation_1-merror:0.20917\n",
      "[20]\tvalidation_0-merror:0.16040\tvalidation_1-merror:0.20943\n",
      "[21]\tvalidation_0-merror:0.15894\tvalidation_1-merror:0.20960\n",
      "[22]\tvalidation_0-merror:0.15707\tvalidation_1-merror:0.20842\n",
      "[23]\tvalidation_0-merror:0.15585\tvalidation_1-merror:0.20749\n",
      "[24]\tvalidation_0-merror:0.15503\tvalidation_1-merror:0.20690\n",
      "[25]\tvalidation_0-merror:0.15417\tvalidation_1-merror:0.20682\n",
      "[26]\tvalidation_0-merror:0.15082\tvalidation_1-merror:0.20572\n",
      "[27]\tvalidation_0-merror:0.14901\tvalidation_1-merror:0.20530\n",
      "[28]\tvalidation_0-merror:0.14771\tvalidation_1-merror:0.20598\n",
      "[29]\tvalidation_0-merror:0.14590\tvalidation_1-merror:0.20741\n",
      "[30]\tvalidation_0-merror:0.14377\tvalidation_1-merror:0.20547\n",
      "[31]\tvalidation_0-merror:0.14209\tvalidation_1-merror:0.20455\n",
      "[32]\tvalidation_0-merror:0.13912\tvalidation_1-merror:0.20522\n",
      "[33]\tvalidation_0-merror:0.13659\tvalidation_1-merror:0.20530\n",
      "[34]\tvalidation_0-merror:0.13567\tvalidation_1-merror:0.20564\n",
      "[35]\tvalidation_0-merror:0.13411\tvalidation_1-merror:0.20404\n",
      "[36]\tvalidation_0-merror:0.13089\tvalidation_1-merror:0.20185\n",
      "[37]\tvalidation_0-merror:0.12910\tvalidation_1-merror:0.20261\n",
      "[38]\tvalidation_0-merror:0.12761\tvalidation_1-merror:0.20219\n",
      "[39]\tvalidation_0-merror:0.12595\tvalidation_1-merror:0.20295\n",
      "[40]\tvalidation_0-merror:0.12391\tvalidation_1-merror:0.20337\n",
      "[41]\tvalidation_0-merror:0.12224\tvalidation_1-merror:0.20337\n",
      "[42]\tvalidation_0-merror:0.12115\tvalidation_1-merror:0.20286\n",
      "[43]\tvalidation_0-merror:0.12031\tvalidation_1-merror:0.20328\n",
      "[44]\tvalidation_0-merror:0.11942\tvalidation_1-merror:0.20370\n",
      "[45]\tvalidation_0-merror:0.11799\tvalidation_1-merror:0.20261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.5, max_delta_step=0, max_depth=7,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=1000, n_jobs=-1, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = ce.OrdinalEncoder()\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_val_encoded = encoder.transform(X_val)\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators = 1000,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "eval_set = [(X_train_encoded, y_train), \n",
    "            (X_val_encoded, y_val)]\n",
    "\n",
    "\"\"\"\n",
    "merror: Multiclass classification error rate. \n",
    "It is calculated as #(wrong cases)/#(all cases).\n",
    "\"\"\"\n",
    "model.fit(X_train_encoded, y_train, \n",
    "          eval_set=eval_set, \n",
    "          eval_metric='merror', \n",
    "          early_stopping_rounds=10) # Stop if the score hasn't improved in 10 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF7-ml6BhRRf"
   },
   "source": [
    "### Try adjusting these hyperparameters\n",
    "\n",
    "#### Random Forest\n",
    "- class_weight (for imbalanced classes)\n",
    "- max_depth (usually high, can try decreasing)\n",
    "- n_estimators (too low underfits, too high wastes time)\n",
    "- min_samples_leaf (increase if overfitting)\n",
    "- max_features (decrease for more diverse trees)\n",
    "\n",
    "#### Xgboost\n",
    "- scale_pos_weight (for imbalanced classes)\n",
    "- max_depth (usually low, can try increasing)\n",
    "- n_estimators (too low underfits, too high wastes time/overfits) — Use Early Stopping!\n",
    "- learning_rate (too low underfits, too high overfits)\n",
    "\n",
    "For more ideas, see [Notes on Parameter Tuning](https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html) and [DART booster](https://xgboost.readthedocs.io/en/latest/tutorials/dart.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will use your portfolio project dataset for all assignments this sprint. Complete these tasks for your project, and document your work.\n",
    "\n",
    "- Continue to clean and explore your data. Make exploratory visualizations.\n",
    "- Fit a model. Does it beat your baseline?\n",
    "- Try xgboost.\n",
    "- Get your model's permutation importances.\n",
    "\n",
    "You should try to complete an initial model today, because the rest of the week, we're making model interpretation visualizations.\n",
    "\n",
    "But, if you aren't ready to try xgboost and permutation importances with your dataset today, you can practice with another dataset instead. You may choose any dataset you've worked with previously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
