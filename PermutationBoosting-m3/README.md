**PermutationBoosting-233:** 

Once again We use Tanzania Water Pump dataset. Data is wrangled to replace zeros with np.nan, features with useless variance are dropped and some new features including datetime type features are engineered. Data is split into train, valid, and test sets. The target label is 'status_group' that has three categorical classes of pump functionality. Train and validation data is grouped into X and y. And test data contains only X features. A pipeline of ordinal encoder, simple imputer and RandomForestClassifier, provides an accuracy score of 0.81. We have three ways to determine important features. We can use classifier .feature_importances_ from sklearn. This will be the fastest method. However, when the dataset has two (or more) correlated features, then from the point of view of the model, any of these correlated features can be used as the predictor, with no concrete preference of one over the others. But once one of them is used, the importance of others is significantly reduced since effectively the impurity they can remove is already removed by the first feature. Hence redundant features will be scored lower. From a model perspective this is ok, however, for the purpose of interpretation of the important features, a lower score for redundant features is misleading. As a second caveat, the scikit-learn Random Forest feature_importances_ tends to inflate the importance of continuous or high-cardinality categorical variables. 

A brute force approach would have been to retrain the model with one feature dropped out and compare the validation score with a time that all the features are included. The difference between validation scores in both cases shows the importance of the examined feature. This fitting process needs to be done for all the features one at a time, which is very slow and costly.

A third approach, Permutation Importance is a good compromise between Feature Importance based on impurity reduction (which is the fastest and in sklearn) and Drop Column Importance (which is the "best, and slow to compute). Here, we train the model once and exclude the features one at a time in the validation data. We won’t be able to drop the feature in validation data as we get size mismatch. Hence we keep the feature but add noise to it such that it becomes useless for the model. This would only work if the distribution of the altered feature is similar to the original one, otherwise the model might fail to produce reliable prediction. To create a similar distribution, we simply shuffle the values of the feature across all observations to nullify its effectiveness. This is called feature permutation. It can be done either manually using np.random.permutation() or using eli5 library to compare the permuted score vs unpermuted score to determine the importance of the feature. PermutationImportance() in eli5 does not take the pipeline as a parameter. Instead it needs the classifier. For that, we need to break up the estimator pipeline into two parts, the transformation part and the classifier part. The steps are as follows. First .fit_transform() the train set for the transformation pipeline to get the X_train_transformed. Then use the .transform() method of the fit pipeline on the validation set to get the X_val_transformed. Next, fit an instance of the classifier for the X_train_transformed. Next instantiate `PermutationImportance(estimator=<trained classifier>)` from eli5.sklearn with the trained classifier as its estimator. Now the permuter instance can be fit with train data or validation data, .fit(X_val_transformed, y_val). We use .feature_importances_ attribute of the permuter or alternatively use `eli5.show_weights(<fit permuter>, ...)` to get a list of important features with their corresponding weights. We can use the top important features for features selection. `mask = permuter.feature_importances_ > minimum_importance; features = X_train.columns[mask]`.

Next part of the notebook uses xgboost for classification of the same dataset. GradientBoost, similar to RandomForest, is a part of the ensemble tree group. In RandomForest We use bagging to do a sample with/out replacement to create multiple bags of train data to train separate trees and then their predictions are aggregated to produce the final ensemble tree prediction. In GradientBoost, the small trees are trained sequentially. We start with a based model, such as majority class or averaging to get an initial prediction for the train labels. Next we calculate 1st pseudo residuals by `y_true - 1st_estimate`. Now we train a tree to predict this residual and add the predicted residual to the 1st_estimate (base model prediction) to get the 2nd_estimate. 2nd_estimate is closer to y_true compared to 1st_estimate as we use the predicted 1st_residual to bring the 2nd_estimate closer to y_true. To lower the variance at the cost of a higher bias, we add a fraction of the predicted residual (learning rate) to the previous estimate. The 2nd_estimate is `1st_estimate + lr * 1st_residual_estimate` Now we’ll calculate the 2nd_residual by `y_true - 2nd_estimate`. A 2nd tree will be trained to predict the 2nd_residual. Now our 3rd_estimate is `1st_estimate + lr * 1st_residual_estimate + lr * 2nd_residual_estimate`. We use our 3rd_estimate to calculate the next residual as `y_true - 3rd_estimate`. This process is continued till we achieve our criteria for loss function or the maximum number of iterations. xgboost() is a variation of the GradientBoost that can accept missing values and enforce monotonic constraints. CatBoost can accept missing values and use categorical features without preprocessing. scikit-learn Gradient Tree Boosting is slower than other libraries, and needs the missing values to be imputed. We build a pipeline of OrdinalEncoder and XGBClassifier(). Fitting and validating the estimator pipeline yield a 0.8 initial accuracy, for Tanzania Water Pump data. With xgboost we can set the number of iteration to a very high value and use early_stopping_rounds parameter for training the model. However, early stopping feature does not work well with pipeline and we need to separate encoder and classifier. Here are the steps: Instantiate ce.OrdinalEncoder(). fit_transform() train data and get the encoded train input. Apply .transform() on validation set to get the encoded validation input. Instantiate XGBClassifier() with all its hyperparameters defined. Finally use the fit() method with eval_set, eval_metric, and early_stopping_rounds set to train the model with early stopping. In this case, the model is trained with encoded train data and validated with encoded validation data. The model will stop if the validation score does not improve in let’s say 10 consecutive attempts.

*Libraries:*
```
import sys
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import category_encoders as ce
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt
import eli5
from eli5.sklearn import PermutationImportance
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
```

In the second notebook, the titanic dataset is used. We use OrdinalEncoder().fit_transform() to convert the categorical features into numeric. However, we leave the missing values in the “Age” column intact and do not impute them. Using cross_val_score() we fit an instance of XGBClassifier(), and get the accuracy score for different folds of the CV. The accuracy score is in the range of 0.8 for the target of “survived” as a binary classifier. Next section is about monotonicity. Having a monotonic relationship between a feature and target usually helps with the interpretation of the model, and finding the impact of individual features on the target. In some cases the noise in the training set in an overit model causes non-monotonic relation. One technique to make the relationship monotonic is to discretize the target value to filter out the noise that causes non monotonic target prediction. Xgboost as a tree-based model uses the following implementation for monotonicity. If for each split of a particular variable we require the right daughter node’s average value to be higher than the left daughter node (otherwise the split will not be made), then approximately this predictor’s relationship with the dependent variable is monotonically increasing; and vice versa. Going to California Housing dataset from sklearn. It has 20K observations. Each observation represents a neighborhood. The response variable is the median housing value in a neighborhood. Predictors include median income, average house occupancy, and location etc. of that neighborhood. We use xgboost library which has its own API. We use .DMatrix() method to convert the train and test data into DMatrix objects. Then we apply the .cv() method to cross validate the train data. We can also use .train() method if we have a split train, test dataset. “params” parameter lets us configure the xgboost for the depth, monotonic constraints and eval_metrics. cv() method allows to set boost_round and early_stopping_round. Every round would have a k-fold CV and gives a row with the score statistic for train and test data. With early stopping we stop if the score doesn’t improve in a certain number of rounds. Without early stopping the training score improves while validation does not. PDPbox only exists in sklearn, we write a function to give us partial dependency of any predictor to the target variable. For calculating the dependency (or partial dependency) of a response variable on a predictor (or multiple predictors) 1. Sample a grid of values of a predictor with linspace() 2. For each value, replace every row of the train data for that particular predictor with this value. We use the same new value from the linear value range for our feature in all the observations, where other feature values change from one observation to another according to the train set. The predicted value .predict() for that feature value would be the aggregated average of all the predictions for that value with all different values of all other features. A plot of x= linear samples of the feature vs y=aggregated prediction for each value of the feature shows the partial dependence plot. Depending on whether the model is trained with monotone_constraints or not we can see the monotonicity behavior on the partial dependence plot.
```
import category_encoders as ce
import pandas as pd
import seaborn as sns
from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets.california_housing import fetch_california_housing
from sklearn.model_selection import train_test_split
import xgboost as xgb
import matplotlib.pyplot as plt
import numpy as np
```

https://github.com/skhabiri/PredictiveModeling-AppliedModeling-u2s3/tree/master/PermutationBoosting-m3
